{"cells":[{"cell_type":"markdown","metadata":{"id":"-sEP512qw-Oc"},"source":["# Information Retrieval\n","## Assignment 8\n","\n","Team members:\n","\n","1: Pouria Sadr\n","\n","2: Kimia Mahdinejad\n","\n","3: Saleh Ebrahimian\n","\n","4: Mobin Tasnimi"]},{"cell_type":"markdown","metadata":{"id":"T4l7dwa8_htt"},"source":["# **Read DataSet**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYk-QZW0xoPT"},"outputs":[],"source":["import pandas as pd\n","from IPython.display import clear_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NLfH3EHPY_U"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","import time\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"SMwgMmt16PgI"},"source":["Persian Content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htMXRa4Rx_Xl"},"outputs":[],"source":["!wget https://github.com/mohamad-dehghani/persian-pdf-books-dataset/raw/master/final_books.xlsx\n","clear_output()\n","df_persian = pd.read_excel('/content/final_books.xlsx')"]},{"cell_type":"markdown","metadata":{"id":"d6v1U17G6ajH"},"source":["English Content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTRwLCuE6WZv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"caf02e31-0889-42fb-d785-4f8b0afd0540","executionInfo":{"status":"ok","timestamp":1682408005430,"user_tz":-210,"elapsed":21242,"user":{"displayName":"Kimia Mn","userId":"02379545362567225986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-04-25 07:33:03--  http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n","Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n","Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 48002242 (46M) [application/x-gzip]\n","Saving to: ‘MovieSummaries.tar.gz’\n","\n","MovieSummaries.tar. 100%[===================>]  45.78M  3.04MB/s    in 18s     \n","\n","2023-04-25 07:33:21 (2.57 MB/s) - ‘MovieSummaries.tar.gz’ saved [48002242/48002242]\n","\n"]}],"source":["!wget http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n","!tar -xf MovieSummaries.tar.gz\n","df_english = pd.read_csv(\"/content/MovieSummaries/plot_summaries.txt\", delimiter = \"\\t\",names=[\"id\",\"text\"])"]},{"cell_type":"markdown","metadata":{"id":"E7KAvKXylyyg"},"source":["# **Prepare DataSet**"]},{"cell_type":"markdown","metadata":{"id":"21l1g1hVl3a5"},"source":["## **Persian**"]},{"cell_type":"markdown","metadata":{"id":"Py_QGj-0e9XV"},"source":["### **Hazm**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xTdtVjsw9QL"},"outputs":[],"source":["!pip install hazm\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZH0wtHCit9-R"},"outputs":[],"source":["import hazm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3XQlxmUccyj"},"outputs":[],"source":["def normalize_and_tokenize_hazm():\n","  # Normalize Data\n","  df_normalized_hazm = []\n","\n","  normalizer = hazm.Normalizer()\n","\n","  for row in df_persian['content']:\n","    if type(row) != str and pd.isna(row):\n","      row = \"\"\n","    df_normalized_hazm.append(normalizer.normalize(row))\n","\n","  # Tokenize Data\n","  df_tokenized_hazm = []\n","\n","  for row in df_normalized_hazm[0:50]:\n","    df_tokenized_hazm.append(hazm.word_tokenize(row))\n","\n","  return df_tokenized_hazm"]},{"cell_type":"markdown","metadata":{"id":"ecvMSSht_2nt"},"source":["### **Parsivar**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75XXHSjV_2MV"},"outputs":[],"source":["!pip install parsivar\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDVIR9SbAxfR"},"outputs":[],"source":["import parsivar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yeoLqK2crYJ"},"outputs":[],"source":["def normalize_and_tokenize_parsivar():\n","  # Normalize Data\n","\n","  df_normalized_parsivar = []\n","\n","  normalizer = parsivar.Normalizer()\n","\n","  for row in df_persian['content']:\n","    if type(row) != str and pd.isna(row):\n","      row = \"\"\n","    df_normalized_parsivar.append(normalizer.normalize(row))\n","\n","  # Tokenize Data\n","  df_tokenized_parsivar = []\n","\n","  tokenizer = parsivar.Tokenizer()\n","\n","  for row in df_normalized_parsivar[0:50]:\n","    df_tokenized_parsivar.append(tokenizer.tokenize_words(row))\n","\n","  return df_tokenized_parsivar"]},{"cell_type":"markdown","metadata":{"id":"ilBjaUN0ciDr"},"source":["### **Dadmatools**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MN8Ju-RtclWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682408094922,"user_tz":-210,"elapsed":44134,"user":{"displayName":"Kimia Mn","userId":"02379545362567225986"}},"outputId":"ea4f8fc5-d344-45f9-957a-3ec0e32e6a73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dadmatools\n","  Downloading dadmatools-1.5.2-py3-none-any.whl (862 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.6/862.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting NERDA\n","  Downloading NERDA-1.0.0-py3-none-any.whl (23 kB)\n","Collecting pytorch-transformers>=1.1.0\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: h5py>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.8.0)\n","Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.5.2)\n","Requirement already satisfied: tabulate>=0.8.6 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.8.10)\n","Requirement already satisfied: gdown>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (4.6.6)\n","Collecting bpemb>=0.3.3\n","  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.4.5)\n","Collecting sklearn>=0.0\n","  Downloading sklearn-0.0.post4.tar.gz (3.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py7zr>=0.17.2\n","  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (2.0.0+cu118)\n","Requirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (4.3.1)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting html2text\n","  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n","Collecting transformers>=4.9.1\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: hyperopt>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.2.7)\n","Collecting supar==1.1.2\n","  Downloading supar-1.1.2-py3-none-any.whl (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Deprecated==1.2.6\n","  Downloading Deprecated-1.2.6-py2.py3-none-any.whl (8.1 kB)\n","Requirement already satisfied: folium>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.14.0)\n","Collecting segtok>=1.5.7\n","  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n","Collecting conllu\n","  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n","Collecting pyconll>=3.1.0\n","  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.9/dist-packages (from Deprecated==1.2.6->dadmatools) (1.14.1)\n","Collecting stanza\n","  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dill\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (1.22.4)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.9/dist-packages (from folium>=0.2.1->dadmatools) (3.1.2)\n","Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from folium>=0.2.1->dadmatools) (0.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (3.11.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (1.16.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (4.11.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim>=3.6.0->dadmatools) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim>=3.6.0->dadmatools) (6.3.0)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.10.9.7)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (3.1)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.18.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (5.9.5)\n","Collecting pybcj>=0.6.0\n","  Downloading pybcj-1.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting texttable\n","  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n","Collecting pyzstd>=0.14.4\n","  Downloading pyzstd-0.15.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (399 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting inflate64>=0.3.1\n","  Downloading inflate64-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pycryptodomex>=3.6.6\n","  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting brotli>=1.0.9\n","  Downloading Brotli-1.0.9-cp39-cp39-manylinux1_x86_64.whl (357 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1\n","  Downloading pyppmd-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multivolumefile>=0.2.3\n","  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (2022.10.31)\n","Collecting boto3\n","  Downloading boto3-1.26.119-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.8)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.4)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.4.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.7)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.3.0)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (0.7.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.8)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.12)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (23.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (67.7.1)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.1.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (8.1.9)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.10.7)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->dadmatools) (2.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->dadmatools) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->dadmatools) (1.11.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.1->dadmatools) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.1->dadmatools) (16.0.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.9.1->dadmatools) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting progressbar\n","  Downloading progressbar-2.5.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from NERDA->dadmatools) (1.5.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.9.1->dadmatools) (2023.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2.0.12)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->dadmatools) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->dadmatools) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->dadmatools) (8.1.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.3.1->dadmatools) (2.4.1)\n","Collecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.119\n","  Downloading botocore-1.29.119-py3-none-any.whl (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->NERDA->dadmatools) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->NERDA->dadmatools) (2022.7.1)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.7.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->pytorch-transformers>=1.1.0->dadmatools) (1.2.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from stanza->supar==1.1.2->dadmatools) (3.20.3)\n","Collecting emoji\n","  Downloading emoji-2.2.0.tar.gz (240 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.1->dadmatools) (1.3.0)\n","Building wheels for collected packages: sklearn, progressbar, sacremoses, emoji\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post4-py3-none-any.whl size=2973 sha256=f2024055564ad383fd193e022c319ec1f95a968f8bcac6635f03a4e7fbef6fb8\n","  Stored in directory: /root/.cache/pip/wheels/d5/b2/a9/590d15767d34955f20a9a033e8db973b79cb5672d95790c0a9\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12080 sha256=94eb0b00fd0a99b5633a54cc988baebbd6385b17956c7983dfc37538f96ab6ac\n","  Stored in directory: /root/.cache/pip/wheels/d7/d9/89/a3f31c76ff6d51dc3b1575628f59afe59e4ceae3f2748cd7ad\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=e194b5bdfc4a71151626bd033285868172913c6ce975891d817f70239af74623\n","  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=3495ca00e9db064566bff7f8782303ba8ebae7fcb26180283f7027f5e2e9db1e\n","  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n","Successfully built sklearn progressbar sacremoses emoji\n","Installing collected packages: tokenizers, tf-estimator-nightly, texttable, sklearn, sentencepiece, progressbar, brotli, segtok, sacremoses, pyzstd, pyppmd, pycryptodomex, pyconll, pybcj, multivolumefile, jmespath, inflate64, html2text, emoji, dill, Deprecated, conllu, py7zr, huggingface-hub, botocore, transformers, s3transfer, bpemb, boto3, stanza, supar, pytorch-transformers, NERDA, dadmatools\n","Successfully installed Deprecated-1.2.6 NERDA-1.0.0 boto3-1.26.119 botocore-1.29.119 bpemb-0.3.4 brotli-1.0.9 conllu-4.5.2 dadmatools-1.5.2 dill-0.3.6 emoji-2.2.0 html2text-2020.1.16 huggingface-hub-0.14.0 inflate64-0.3.1 jmespath-1.0.1 multivolumefile-0.2.3 progressbar-2.5 py7zr-0.20.5 pybcj-1.0.1 pyconll-3.1.0 pycryptodomex-3.17 pyppmd-1.0.0 pytorch-transformers-1.2.0 pyzstd-0.15.7 s3transfer-0.6.0 sacremoses-0.0.53 segtok-1.5.11 sentencepiece-0.1.98 sklearn-0.0.post4 stanza-1.5.0 supar-1.1.2 texttable-1.6.7 tf-estimator-nightly-2.8.0.dev2021122109 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install dadmatools\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opuEgmy8csEq"},"outputs":[],"source":["import dadmatools\n","def normalize_and_tokenize_dadmatools():\n","  from dadmatools.models.normalizer import Normalizer\n","  import dadmatools.pipeline.language as language\n","\n","  # Normalize Data\n","  df_normalized_dadmatools = []\n","\n","  normalizer = Normalizer()\n","\n","  for row in df_persian['content']:\n","    if type(row) != str and pd.isna(row):\n","      row = \"\"\n","    df_normalized_dadmatools.append(normalizer.normalize(row))\n","\n","  # Tokenize Data\n","  df_tokenized_dadmatools = [[]]\n","\n","  pips = 'tok'\n","  nlp = language.Pipeline(pips)\n","  clear_output()\n","\n","  counter = 0\n","\n","  for row in df_normalized_dadmatools[0:50]:\n","    if row != '':\n","      doc = nlp(row[0:len(row)-1])\n","      for token in doc:\n","        df_tokenized_dadmatools[counter].append(str(token))\n","    else:\n","      df_tokenized_dadmatools[counter].append(\"\")\n","    counter += 1\n","    df_tokenized_dadmatools.append([])\n","\n","  return df_tokenized_dadmatools"]},{"cell_type":"markdown","metadata":{"id":"lHUAefxYmP4b"},"source":["### **Comparison**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkQ7l-yQfFs5","outputId":"a58e88d8-4827-4fc1-d5e3-f6ddad737a73","executionInfo":{"status":"ok","timestamp":1682408098513,"user_tz":-210,"elapsed":3605,"user":{"displayName":"Kimia Mn","userId":"02379545362567225986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Hazm - Execution Time:  2.29205322265625\n","Hazm - Tokens Found:  128\n"]}],"source":["start_time = time.time()\n","df_normalized_tokenized_hazm = normalize_and_tokenize_hazm()\n","print(\"Hazm - Execution Time: \", time.time() - start_time)\n","print(\"Hazm - Tokens Found: \", len(df_normalized_tokenized_hazm[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6r1i6Oepkioa","outputId":"fa815a23-6616-4b6a-fa83-e2c15e71a304","executionInfo":{"status":"ok","timestamp":1682408104514,"user_tz":-210,"elapsed":6010,"user":{"displayName":"Kimia Mn","userId":"02379545362567225986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Parsivar - Execution Time:  6.147846221923828\n","Parsivar - Tokens Found:  129\n"]}],"source":["start_time = time.time()\n","df_normalized_tokenized_parsivar = normalize_and_tokenize_parsivar()\n","print(\"Parsivar - Execution Time: \", time.time() - start_time)\n","print(\"Parsivar - Tokens Found: \", len(df_normalized_tokenized_parsivar[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8EOM3yl6lB8J","outputId":"ad701c74-61dc-4a09-bdcb-303dc3bf6594","executionInfo":{"status":"ok","timestamp":1682408138172,"user_tz":-210,"elapsed":33669,"user":{"displayName":"Kimia Mn","userId":"02379545362567225986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Dadmatools - Execution Time:  33.74518942832947\n","Dadmatools - Tokens Found:  134\n"]}],"source":["start_time = time.time()\n","df_normalized_tokenized_dadmatools = normalize_and_tokenize_dadmatools()\n","print(\"Dadmatools - Execution Time: \", time.time() - start_time)\n","print(\"Dadmatools - Tokens Found: \", len(df_normalized_tokenized_dadmatools[0]))"]},{"cell_type":"markdown","metadata":{"id":"rEtu6YwrmCxO"},"source":["## **English**"]},{"cell_type":"markdown","metadata":{"id":"ncBs3Qmybz9K"},"source":["### **NLTK**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdc-t08duABp"},"outputs":[],"source":["import nltk\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-r79sWM9vL7K"},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fWXLm4Dc-RR"},"outputs":[],"source":["def normalize_and_tokenize_nltk():\n","  # Normalize Data\n","\n","  df_normalized_nltk = []\n","\n","  for row in df_english['text']:\n","    if type(row) != str and pd.isna(row):\n","      row = \"\"\n","    df_normalized_nltk.append(re.sub(r'[^\\w\\s]','',row))\n","\n","  # Tokenize Data\n","  df_tokenized_nltk = []\n","\n","  for row in df_normalized_nltk[0:50]:\n","    df_tokenized_nltk.append(nltk.word_tokenize(row))\n","\n","  return df_tokenized_nltk"]},{"cell_type":"markdown","metadata":{"id":"3TPTaRKZeIQw"},"source":["### **Spacy**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pa5GEwIzeHir"},"outputs":[],"source":["!pip install spacy\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3HAhayTeK8j"},"outputs":[],"source":["import spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rylRn_HudEon"},"outputs":[],"source":["def normalize_and_tokenize_spacy():\n","  # Normalize Data\n","\n","  df_normalized_spacy = []\n","\n","  for row in df_english['text']:\n","    if type(row) != str and pd.isna(row):\n","      row = \"\"\n","    df_normalized_spacy.append(re.sub(r'[^\\w\\s]','',row))\n","\n","  # Tokenize Data\n","  df_tokenized_spacy = [[]]\n","\n","  pips = 'tok'\n","  nlp = spacy.load('en_core_web_sm')\n","  clear_output()\n","\n","  counter = 0\n","\n","  for row in df_normalized_spacy[0:50]:\n","    if row != '':\n","      doc = nlp(row[0:len(row)-1])\n","      for token in doc:\n","        df_tokenized_spacy[counter].append(str(token))\n","    else:\n","      df_tokenized_spacy[counter].append(\"\")\n","    counter += 1\n","    df_tokenized_spacy.append([])\n","\n","  return df_tokenized_spacy"]},{"cell_type":"markdown","metadata":{"id":"Bi2pUE8DmiDq"},"source":["### **Comparison**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkWtcoidlR58","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df829945-8040-43ab-c4fd-99f5642c0e79","executionInfo":{"status":"ok","timestamp":1682408148081,"user_tz":-210,"elapsed":1956,"user":{"displayName":"Kimia Mn","userId":"02379545362567225986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK - Execution Time:  1.8857836723327637\n","NLTK - Tokens Found:  25\n"]}],"source":["start_time = time.time()\n","df_normalized_tokenized_nltk = normalize_and_tokenize_nltk()\n","print(\"NLTK - Execution Time: \", time.time() - start_time)\n","print(\"NLTK - Tokens Found: \", len(df_normalized_tokenized_nltk[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiqxCXS_ln20","colab":{"base_uri":"https://localhost:8080/"},"outputId":"00c02b46-c486-4d69-aee0-847435a71470","executionInfo":{"status":"ok","timestamp":1682408158676,"user_tz":-210,"elapsed":10603,"user":{"displayName":"Kimia Mn","userId":"02379545362567225986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["SpaCy - Execution Time:  10.816453695297241\n","SpaCy - Tokens Found:  26\n"]}],"source":["start_time = time.time()\n","df_normalized_tokenized_spacy = normalize_and_tokenize_spacy()\n","print(\"SpaCy - Execution Time: \", time.time() - start_time)\n","print(\"SpaCy - Tokens Found: \", len(df_normalized_tokenized_spacy[0]))"]},{"cell_type":"markdown","source":["Filter Stop Words"],"metadata":{"id":"HnPnMmnCcSKo"}},{"cell_type":"code","source":["def filter_stop_words(language_type, df_normalized_and_tokenized):\n","  if language_type == \"persian\":\n","    stopwords = hazm.utils.stopwords_list()\n","    stopwords.append(\".\")\n","    stopwords.append(\"،\")\n","  elif language_type == \"english\":\n","    stopwords = set(nltk.corpus.stopwords.words('english'))\n","\n","  df_filtered = []\n","\n","  for sentence in df_normalized_and_tokenized:\n","    filtered = [w for w in sentence if not w in stopwords]\n","    df_filtered.append(filtered)\n","\n","  return df_filtered"],"metadata":{"id":"A5VHv0UMcU9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TF-IDF**"],"metadata":{"id":"xWnzXtZzWWT9"}},{"cell_type":"markdown","source":["## Persian"],"metadata":{"id":"Deq5xjm_WejY"}},{"cell_type":"code","source":["df_filtered_hazm = filter_stop_words(\"persian\", df_normalized_tokenized_hazm)\n","import numpy as np"],"metadata":{"id":"vLOzyoh2Wodo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **TF**"],"metadata":{"id":"koGh2O4Reeho"}},{"cell_type":"code","source":["def CalcTF(docs , words_set):\n","  n_docs = len(docs)         #·Number of documents in the corpus\n","  n_words_set = len(words_set) #·Number of unique words in the\n","\n","  df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_set)\n","\n","\n","  for i in range(n_docs):\n","      words = docs[i]\n","      for w in words:\n","          df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n","\n","  return df_tf"],"metadata":{"id":"T-k4IXuHd-lB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **IDF**"],"metadata":{"id":"MWEIiZHtm76N"}},{"cell_type":"code","source":["def CalcIDF(docs , words_set):\n","\n","  n_docs = len(docs)\n","  idf = {}\n","  for w in words_set:\n","      k = 0\n","      for i in range(n_docs):\n","          if w in docs[i]:\n","              k += 1\n","\n","      idf[w] =  np.log10(n_docs / k)\n","\n","  return idf"],"metadata":{"id":"C_G1OjtIm5Jx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **TF_IDF**"],"metadata":{"id":"cCMUHCCFm-VE"}},{"cell_type":"code","source":["def CalcTF_IDF(docs):\n","  n_docs = len(docs)\n","\n","  words_set = set()\n","  for doc in docs:\n","    words_set = words_set.union(set(doc))\n","\n","  df_tf = CalcTF(docs,words_set)\n","  idf = CalcIDF(docs,words_set)\n","\n","  df_tf_idf = df_tf.copy()\n","\n","  for w in words_set:\n","      for i in range(n_docs):\n","          df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n","\n","  return df_tf , idf , df_tf_idf"],"metadata":{"id":"DI5RVm3ym6To"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Search in Docs**"],"metadata":{"id":"p9BLIptrnELF"}},{"cell_type":"code","source":["def Search(tokenized_query , docs , docstfidf):\n","  doc_len = len(docs)\n","  df_similarity = pd.DataFrame(np.zeros((doc_len, len(tokenized_query))), columns=tokenized_query)\n","\n","  for i in range(len(docs)):\n","    doc = docstfidf.iloc[i]\n","    doc_sum = 0\n","    for word in tokenized_query:\n","      if word in doc:\n","        doc_sum += doc[word]\n","        df_similarity[word][i] = doc[word]\n","\n","\n","  df_similarity['Sum'] = df_similarity.sum(axis=1)\n","  print(df_similarity)\n","\n","  value=df_similarity['Sum'].idxmax()\n","  return value , persian_docs[value]"],"metadata":{"id":"goF-kOWBgoyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["persian_docs = df_filtered_hazm[:100]\n","tf , idf , tfidf = CalcTF_IDF(persian_docs)"],"metadata":{"id":"xJhyOCUJnNlI","colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"status":"error","timestamp":1682408546722,"user_tz":-210,"elapsed":10,"user":{"displayName":"Kimia Mn","userId":"02379545362567225986"}},"outputId":"9015b396-77fd-4a34-9f87-6f119d2e03ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-79cd36a5494a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpersian_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filtered_hazm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalcTF_IDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersian_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-41-2647938d8ebe>\u001b[0m in \u001b[0;36mCalcTF_IDF\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwords_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mdf_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalcTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalcIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-6fffb157fca6>\u001b[0m in \u001b[0;36mCalcTF\u001b[0;34m(docs, words_set)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mdf_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"columns cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: columns cannot be a set"]}]},{"cell_type":"code","source":["query = \"سخنرانی انگیزشی\";\n","normalizer = hazm.Normalizer()\n","normilized_query = normalizer.normalize(query)\n","tokenized_query = hazm.word_tokenize(normilized_query)\n","\n","result = Search(tokenized_query , persian_docs , tfidf)\n","result"],"metadata":{"id":"Djdko14qwP-P"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["ecvMSSht_2nt","ilBjaUN0ciDr","lHUAefxYmP4b","ncBs3Qmybz9K","3TPTaRKZeIQw","MWEIiZHtm76N","cCMUHCCFm-VE"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}